# Home-Credit-Default
Feature Engineering techniques for dealing Categorical variables
The Data
The dataset was obtained from a recent Kaggle competition. It was supplied by Home Credit, a financial institution that provides services to unbanked population, who has insufficient financial records for lenders to consider a loan. The dataset contains 7 tables with a total of 212 attributes and over 20 million records. Some characteristics of the dataset include: 
1.	Highly imbalanced distribution of the target variables. The default class takes up about 10% of the overall data. 
2.	In the training dataset provided, categorical variables with discrete and qualitative values takes up to almost 50% of all variables. 
Review of Literature 
Target encoding.
Micci-Barreca discussed this approach in dealing with high-cardinality categorical attributes in his studies (Micci-Barreca, 2001). This approach is based on a well- established statistical method - empirical Bayes. The basic idea is to map individual values of a high-cardinality categorical predictor to an estimate of the probability or the expected value of the target variable. After this process, the output of the encoding will maintain the original dimension, thus, the model for the prediction or classification will not need to take on the burden of manipulation of extremely large matrices. He also specified steps in the paper for binary targets: individual values Xi of a categorical feature X is mapped to a scalar Si that represents the probability of Xi in X given Y equals to target. Si is determined by the portion of the number of the value Xi that under the same target, in the case of an even distribution of all the Xi in X. While the distribution is not even, both the posterior and prior probabilities will be added up together by a weighting factor. The advantage of the method is that it is easy to understand and apply. The main drawback of this approach is that it may cause overfitting as the encoded variables are correlated with the target. However, this can be mitigated by adding random noises.  
Entity Embedding Neural Networks.
Entity embeddings are commonly used in unstructured data, such as image recognition and natural language processing. Recently, researchers have started to look into the possibility of using entity embeddings with structured data, and experiments shown good results. Guo et al. used entity embedding neural networks to handle categorical data to discover the continuity representation of categorical variables (Guo & Berkhahn, 2016). They applied entity embedding layers on top of the one-hot encoding layer, and the weights connecting the one-hot encoding layer to the embedding layer are then fed into a layer of linear neurons to generate the final output. One thing that worth noting is that the dimensions of embedding layers are considered as hyper-parameters and thus needed to be pre-defined. The possible number of dimensions of embedding layers ranges between 1 and mi, where mi is the values for categorical variable xi. The choice of the number of dimensions follows empirical guidelines. mi -1 is a good start when lack of sufficient guidelines. In another study, Brébisson et al. applied entity embedding to solving a taxi destination prediction problem from Kaggle. They mapped nominal variables, such as Client ID, Taxi ID and Week of the year, to a 10-dimention vector, the embeddings were first randomly initialized, and then modified by the training algorithm like the other parameters. 
After this, these embeddings were concatenated to the rest of data to form the input vector of the MLP. I (Brébisson & Simon, 2016).  The main benefit of using entity embedding for categorical variables is that it maintains the inherent relationships between entity, and not like any other methods that isolate each value of the categorical variable after the transformation.   
  
Recursive Feature Elimination.
RFE is a widely used wrapper class for feature selection as it selects the best features based on the scores the underlying model provided. It works well with mainstream machine learning models, such as linear regression, LG, RF and SVM and so on. Samb et al. studied the RFE with SVM algorithm and improved the performance by combing the RFESVM with local search, which enhances the quality of the initial solution provided by the algorithm. 
Their experimental results shown that the reuse of features previously removed during the RFESVM process can improve the quality of the final classifier (Samb, Camara, & Ndiaye, 2012). Compared to SVM, which is s more suitable method for binary classification, RF is a natural multiclass algorithm with an internal (unbiased) measure of features importance (Granitto, 
Furlanellob, & Biasioli, 2006). Granitto et al. studied the performance between RFE-SVM and RFE-RF on PTR-MS data sets and results shown that RFE-RF outperforms RFE-SVM on finding small subsets of features with high discrimination levels on the data sets. However, RFE tends to discard "weak" features, which may provide a significant improvement of performance when combined with other features (Chen & Jeong, 2007). 
